import logging
from logging.handlers import RotatingFileHandler
from datetime import datetime
import magic
import os
import requests
from bs4 import BeautifulSoup
from zipfile import ZipFile
from tqdm import tqdm
from urllib.parse import urljoin
import filetype
from config import ROOT_DIR


# Configure the logging settings
logging.basicConfig(level=logging.DEBUG)  # You can adjust the logging level as needed

def setup_logger():
    # Create a logger
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)

    # Create a formatter with a custom date format
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')

    log_file_path = ROOT_DIR + '/logs/malware-analysis-net.log'
    # Create a file handler to log messages to a file
    file_handler = RotatingFileHandler(log_file_path, maxBytes=1000000, backupCount=3)
    file_handler.setFormatter(formatter)
    
    stream_handler = logging.StreamHandler()
    stream_handler.setFormatter(formatter)

    # Add the file handler to the logger
    logger.addHandler(file_handler)
    logger.addHandler(stream_handler)

    return logger

logger = setup_logger()


def is_file_type(file_path: str, expected_type: str):
    if not os.path.isfile(file_path):
        logger.info('File does not exists')
        return False
    try:
        kind = filetype.guess(file_path)
    except Exception as e:
        logger.info(e)
    if kind is None:
        logger.info(f"Cannot get filetype of {file_path}")
        return False
    logger.info(f"Real file extension: {kind.extension}")
    logger.info(f"Real MIME type: {kind.mime}")
    return kind.extension == expected_type

def mime_type_with_magic(file_path):
    mime = magic.Magic()
    return mime.from_file(file_path)

def is_valid_path(path, required='pcap'):
    """
    Check if a file path is valid and does not contain relative parent directory references.

    Args:
    - path (str): The file path to check.

    Returns:
    - bool: True if the path is valid, False otherwise.
    """

    path = os.path.normpath(path)
    sanity_list = ['..', '//']
    for check in sanity_list:
        if check in path:
            return False
    
    return required in path

def download_file_from_web(url, **kwargs):
    # Send a GET request to the URL
    response = requests.get(url)

    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')

        # Find all links with the class 'menu_link'
        links = soup.find_all('a', class_='menu_link')


        # Iterate over each link and download/extract the corresponding zip file
        for link in links:
            # Get the href attribute of the link
            file_url = urljoin(url, link['href'])

            # Extract the file name from the URL
            file_name: str = os.path.basename(file_url)
            
            # Create a directory to store the downloaded and extracted files
            output_directory = kwargs.get('output_directory')
            os.makedirs(output_directory, exist_ok=True)
            pcap_file_name = os.path.join(output_directory, file_name.removesuffix('.zip'))
            if is_existed_file(pcap_file_name) or is_existed_folder(pcap_file_name):
                logger.error(f'File {file_name} have already been downloaded')
                return False

            # Specify the local path to save the downloaded zip file
            local_file_path = os.path.join(output_directory, file_name)

            if not is_valid_path(local_file_path):
                logger.error(f"Path is not valid: {local_file_path}")
                continue
            # Download the zip file
            with open(local_file_path, 'wb') as file:
                logger.info(f"Downloading zip file from {file_url}")
                try:
                    file.write(requests.get(file_url).content)
                except Exception as e:
                    logger.info(e)
                
            logger.info(f"Saved {file_name} to {local_file_path}")
            extract_zipped_file(local_file_path, **kwargs)
    else:
        logger.info(f"Failed to retrieve the webpage. Status code: {response.status_code}")

def extract_zipped_file(file_path, **kwargs):
    if not is_file_type(file_path, 'zip'):
        return False
    # Create a directory to store the downloaded and extracted files
    output_directory = kwargs.get('output_directory')
    os.makedirs(output_directory, exist_ok=True)
    file_name: str = os.path.basename(file_path)
    
    # Store extracted file to folder with same filename
    output_directory = os.path.join(output_directory, file_name.removesuffix('.zip'))
    logger.info(output_directory)
    try:
        os.makedirs(output_directory, exist_ok=True)
    except Exception as e:
        logger.info(e)
    # Extract the contents of the zip file
    try:
        with ZipFile(file_path, 'r') as zip_ref:
            zip_ref.extractall(output_directory, pwd=kwargs.get('pwd').encode())
    except Exception as e:
        logger.info(e)
        
    logger.info(f"Extracted {file_name} to {output_directory}")
    os.remove(file_path)
    logger.info(f"Remove {file_path}")


def download_multiple_links(urls, **kwargs):
    for url in tqdm(urls):
        download_file_from_web(url, **kwargs)

def retrieve_all_links_from_url(url, class_tag, tag):
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        links = soup.find_all(tag, class_=class_tag)   
        real_url = []
        for link in links:
            real_url.append(urljoin(url, link['href']))
    return real_url

def is_existed_file(file_path: str):
    '''
    Return `True` if `file_path` is existed else `False`
    '''
    return os.path.isfile(file_path)

def is_existed_folder(file_path: str):
    return os.path.isdir(file_path)
