import os
import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import argparse
import logging
from logging.handlers import RotatingFileHandler
from tqdm import tqdm
import humanize
from colorlog import ColoredFormatter
import sys
sys.path.insert(0, os.getcwd())
from config import ROOT_DIR

PACKAGE_MAX_SIZE = 1 * 1000 * 1000 * 1000

# Configure the logging settings
logging.basicConfig(level=logging.DEBUG)  # You can adjust the logging level as needed

def setup_logger():
    # Create a logger with the name 'example_function'
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)

    # Create a formatter with a custom date format
    formatter = ColoredFormatter(
    "%(log_color)s%(levelname)-8s%(reset)s%(white)s%(asctime)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    reset=True,
    log_colors={
        'DEBUG': 'cyan',
        'INFO': 'green',
        'WARNING': 'yellow',
        'ERROR': 'red',
        'CRITICAL': 'red,bg_white',
    },
    secondary_log_colors={},
    style='%'
)
    log_file_path = ROOT_DIR + '/logs/ctu-13_dowload.log'
    # Create a file handler to log messages to a file
    file_handler = RotatingFileHandler(log_file_path, maxBytes=1000000, backupCount=3)
    file_handler.setFormatter(formatter)
    
    stream_handler = logging.StreamHandler()
    stream_handler.setFormatter(formatter)

    # Add the file handler to the logger
    logger.addHandler(file_handler)
    logger.addHandler(stream_handler)

    return logger

logger = setup_logger()

def download_file(url, destination):
    '''
    Download file from `url` to `destination` using `Stream Mode`
    Return: `None`
    '''
    partial_destination = f"{destination}.partial"
    response = requests.get(url, stream=True)
    total_size = int(response.headers.get('content-length', 0))
    if args.limit:
        if total_size > PACKAGE_MAX_SIZE:
            logger.info(f"File {os.path.basename(destination)} too huge ({humanize.naturalsize(total_size)})")
            with open('huge_files.txt', 'a') as f:
                f.write(url + ' ' +  humanize.naturalsize(total_size)+'\n')
            return
    logger.info(f"File {os.path.basename(url)} has size of {humanize.naturalsize(total_size)} bytes")
    try:
        with open(partial_destination, 'wb') as file, tqdm(
            desc=os.path.basename(partial_destination),
            total=total_size,
            unit='B',
            unit_scale=True,
            unit_divisor=1024,
        ) as bar:
            for data in response.iter_content(chunk_size=1024):
                bar.update(len(data))
                file.write(data)
    except KeyboardInterrupt as e:
        logger.info(f"Download interrupted. Removing partial file: {partial_destination}")
        os.remove(partial_destination)
        exit(0)
    os.rename(partial_destination, destination)
    logger.info(f"Downloaded {destination}")
    
def is_existed_file(file_path: str):
    '''
    Return `True` if `file_path` is existed else `False`
    '''
    return os.path.isfile(file_path)

def download_files(urls: list, dir_name: str) -> None:
    '''
    Download all file in `urls` into `dir_name`
    Return: `None`
    '''
    for url in urls:
        file_path = os.path.join(dir_name, os.path.basename(url))
        if is_existed_file(file_path):
            continue
        download_file(url, file_path)
        
def find_all_links(base_url: str, substring: str) -> list:
    '''
    Find all links in html of `base_url` containing `substring`
    return: `links`: list
    '''
    response = requests.get(base_url)
    soup = BeautifulSoup(response.text, 'html.parser')
    links = soup.find_all('a', href=re.compile(re.escape(substring))) # return `a` tag
    links = [link['href'] for link in links]
    return links 

def find_all_links_with_suffix(base_url: str, suffix: str) -> list:
    logger.info(f"Checking {base_url} for {suffix} files...")
    # Get links from the page_url
    page_response = requests.get(base_url)
    page_soup = BeautifulSoup(page_response.text, 'html.parser')
    file_links = page_soup.find_all('a', href=re.compile(f'{re.escape(suffix)}$')) # return `a` tag
    file_links: list[str] = [link['href'] for link in file_links]
    for index, file_link in enumerate(file_links):
        if not file_link.startswith("https://"):
            file_links[index] = urljoin(base_url, file_link)
    return file_links

def find_and_download_files(base_url: str, substring: str, file_suffix: str, download_folder='../downloads') -> None:
    '''
    Find all links in html of `base_url` containing `substring` with `find_all_links`
    And in each link, find the url have the `file_suffix` to download into `download_folder`
    return: `None`
    '''
    
    links = find_all_links(base_url, substring)

    for link in links:
        page_url = ''
        file_links = find_all_links_with_suffix(link, file_suffix)
        
        for file_link in file_links:
            file_url = urljoin(page_url, file_link)
            file_name = os.path.basename(file_url)
            destination_path = os.path.join(download_folder, file_name)
            # Download the file
            if is_existed_file(destination_path):
                logger.info(f"File {file_name} already in {destination_path}")
                continue
            logger.info(f'Downloading {file_name} from {file_url}...')
            download_file(file_url, destination_path)

if __name__ == '__main__':
    parser = argparse.ArgumentParser("Download help for CTU13 and MCFP")
    parser.add_argument("--folder", help="Download folder name")
    parser.add_argument("--url", help="Base url")
    parser.add_argument("--substr", help="Substr in url")
    parser.add_argument("--suffix", help="Suffix of file, ex: .pcap, .txt")
    parser.add_argument("--file", help="File contains urls to download")
    parser.add_argument("--limit", help="Enable limit size")
    parser.add_argument("--cmd", help="List and file have suffix")
    
    args = parser.parse_args()
    if len(args._get_args()) < 1:
        parser.print_help()
        exit(0)
    if not args.url:
        base_url = 'https://www.stratosphereips.org/datasets-malware'  # Replace with the URL of the HTML file
    else:
        base_url = args.url
    
    if not args.substr:
        substring_to_find  = 'CTU-Malware-Capture-Botnet'  # Replace with the specific prefix you are looking for
    else:
        substring_to_find  = args.substr
    
    if not args.suffix:    
        file_suffix = '.pcap'  # Replace with the desired suffix (e.g., '.txt', '.dat', etc.)
    else:
        file_suffix = args.suffix
        
    if not args.folder:
        download_folder = '../downloads'
    else:
        download_folder = args.folder
        
    if args.cmd == 'list':
        links = find_all_links(base_url, substring_to_find)
        all_links = []
        for link in links:
            logger.info(f"Url {link} have the following files:")
            result_links = find_all_links_with_suffix(link, file_suffix)
            logger.info(result_links)
            all_links.extend(result_links)
        open("links.py", "w").write(str(all_links))
        exit(0)
    
    if args.file:    
        urls = open(args.file, "r").readlines()
        urls = tuple(map(lambda url: url.split()[0], urls))
        download_files(urls, download_folder)
    else:
        find_and_download_files(base_url, substring_to_find , file_suffix, download_folder)
