import argparse
import sys
import os

sys.path.insert(0, os.getcwd())
from utils.modules import *

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--password', '-pwd', help='Password is used to extract zip file')
    parser.add_argument('--url', '-u', help='Url of web contains download links')
    parser.add_argument('--file', '-f', help='File contains links')
    parser.add_argument('--debug', action='store_false', help='Enable debug mode')
    parser.add_argument('--folder', '-fd', default='.', help='Folder to save downloaded files')
    parser.add_argument('-sy', help="Start year", type=int, default=2013)
    parser.add_argument('-ey', help="End year", type=int, default=2024)
    
    args = parser.parse_args()
    if len(args._get_args()) < 1:
        parser.print_help()
        exit(0)
    if not args.debug:
        if not args.url or not args.file:
            parser.print_help()
            exit(0)
    elif args.url:
        print(args.url)
        download_file_from_web(args.url, pwd=args.password, output_directory=args.folder)
        exit(0)
    elif not args.file:
        url_formats = [f'https://malware-traffic-analysis.net/{year}/index.html' for year in range(args.sy, args.ey)]
        urls = [retrieve_all_links_from_url(url_format, 'list_header', 'a') for url_format in url_formats]
        urls = sum(urls, [])
        logger.info(urls)
        with open('urls.txt', 'w') as file:
            file.write(urls.__str__())
    else:
        urls = eval(open(args.file).read()) # critical security
    # URL of the web page containing the links to the zip files
    download_multiple_links(urls, pwd=args.password, output_directory=args.folder)
